{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-31T13:15:52.602761Z",
     "end_time": "2023-05-31T13:15:52.607596Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/matteoferla/c7d28da213dc4c787aad6636f745c2b7#file-fragnet_merging-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "from rdkit import Chem\n",
    "\n",
    "def make_fauxalysis(hits: List[Chem.Mol], target_name: str, base_folder='workshop') -> None:\n",
    "    \"\"\"\n",
    "    VANDALISED. SEE ORIGINAL GIST\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.mkdir(base_folder)\n",
    "    os.environ['FRAGALYSIS_DATA_DIR'] = base_folder\n",
    "    for hit in hits:\n",
    "        hit_name: str = hit.GetProp('_Name')\n",
    "        hit_path = os.path.join(base_folder, 'temp', 'aligned', hit_name)\n",
    "        os.makedirs(hit_path, exist_ok=True)\n",
    "        Chem.MolToMolFile(hit, os.path.join(hit_path, f'{target_name}-{hit_name}.mol'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle, gzip\n",
    "from typing import Dict, List\n",
    "\n",
    "# -------------------------------\n",
    "## Import hits\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "with Chem.SDMolSupplier('filtered_hits.sdf') as sds:\n",
    "    hits: List[Chem.Mol] = list(sds)\n",
    "\n",
    "hit_smileses = [Chem.MolToSmiles(hit) for hit in hits]\n",
    "hit_names = [hit.GetProp('_Name') for hit in hits]\n",
    "hitdex = dict(zip(hit_names, hits))\n",
    "target_name = 'ðŸ‘¾ðŸ‘¾ðŸ‘¾'\n",
    "make_fauxalysis(hits, target_name, os.path.join(os.getcwd(), 'fauxalysis'))\n",
    "\n",
    "# -------------------------------\n",
    "## Search\n",
    "\n",
    "from merge import query\n",
    "from merge.find_merges import getFragmentNetworkSearcher\n",
    "from merge.find_merges_generic import MergerFinder_generic  # solely for typehinting\n",
    "\n",
    "searcher: MergerFinder_generic = getFragmentNetworkSearcher()\n",
    "\n",
    "valid_smileses, valid_names = searcher.filter_for_nodes(hit_smileses, hit_names)\n",
    "smiles_pairs, name_pairs = searcher.get_combinations(valid_smileses, valid_names)\n",
    "all_mergers: List[Dict] = []\n",
    "for smiles_pair, name_pair in zip(smiles_pairs, name_pairs):\n",
    "    mergers: Dict[str, List[str]] = searcher.get_expansions(smiles_pair, name_pair, target_name, 'output')\n",
    "    all_mergers.append(dict(mergers=mergers, smiles_pair=smiles_pair, name_pair=name_pair))\n",
    "\n",
    "with gzip.open('fragmented.pkl.gz', 'wb') as fh:\n",
    "    pickle.dump(all_mergers, fh)\n",
    "\n",
    "print(len(all_mergers),\\\n",
    "sum([len(m['mergers']) for m in all_mergers]), \\\n",
    "sum([len(mm) for m in all_mergers for mm in m['mergers'].values()]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Parse synthons\n",
    "\n",
    "import operator\n",
    "import pandas as pd\n",
    "from fragmenstein import Victor, Laboratory, Igor\n",
    "\n",
    "dfs = [ pd.DataFrame([{'smiles': synthon.replace('Xe', 'H'),\n",
    "                       'original_name': f'{merge_info[\"name_pair\"][1]}-synthon{i}',\n",
    "                       'xenonic': synthon,\n",
    "                       'parent': merge_info['name_pair'][1],\n",
    "                       'hits': [hitdex[merge_info['name_pair'][1]]]} for i, synthon in enumerate(merge_info['mergers'].keys())])\n",
    "       for merge_info in all_mergers\n",
    "      ]\n",
    "\n",
    "synthons = pd.concat(dfs, axis='index')\n",
    "\n",
    "# fix duplicated\n",
    "synthons['inchi'] = synthons.smiles.apply(Chem.MolFromSmiles).apply(Chem.RemoveAllHs).apply(Chem.MolToInchiKey)\n",
    "synthons = synthons.drop_duplicates(['parent', 'inchi'])\n",
    "synthons['name'] = synthons.parent +'Â§'+ (synthons.groupby(['parent']).cumcount()+1).astype(str)\n",
    "Igor.init_pyrosetta()\n",
    "placed_synthons = Laboratory(pdbblock=pdb_block, covalent_resi=None).place(synthons, n_cores=2)\n",
    "\n",
    "def fix_name(row):\n",
    "    # error... min_mol has it. not unmin.\n",
    "    mol = Chem.Mol(row.unmin_binary)\n",
    "    mol.SetProp('_Name', row['name'])\n",
    "    return mol\n",
    "\n",
    "synthons['âˆ†âˆ†G'] = placed_synthons['âˆ†âˆ†G']\n",
    "synthons['unmin_mol'] = placed_synthons.apply(fix_name, axis=1)\n",
    "from rdkit.Chem import PandasTools\n",
    "PandasTools.WriteSDF(df=synthons,\n",
    "                     out='synthons.sdf',\n",
    "                     molColName='unmin_mol',\n",
    "                     idName='name',\n",
    "                     properties=['parent', 'âˆ†âˆ†G'])\n",
    "## --------------------------------------\n",
    "# fix names of synthons in combination and make it a long table\n",
    "data = []\n",
    "combodex: dict\n",
    "for combodex in all_mergers:\n",
    "    # 'mergers', 'smiles_pair', 'name_pair'\n",
    "    first_name, second_name = combodex['name_pair']\n",
    "    first: Chem.Mol = hitdex[first_name]\n",
    "    for synthon_smiles in combodex['mergers']:\n",
    "        clean_smiles = synthon_smiles.replace('Xe', 'H')\n",
    "        inchi = Chem.MolToInchiKey( Chem.RemoveAllHs( Chem.MolFromSmiles(clean_smiles) ) )\n",
    "        matched = placed_synthons.loc[(placed_synthons['parent'] == second_name) & (placed_synthons.inchi == inchi)]\n",
    "        if len(matched) == 0:\n",
    "            print(first_name, second_name, synthon_smiles, 'missing!')\n",
    "            # Z2111637360\n",
    "            second = hitdex[second_name]\n",
    "            synthon_name = second_name+'Â§X'\n",
    "        elif matched.iloc[0]['âˆ†âˆ†G'] > -1.:\n",
    "            # skip crap floater fragments\n",
    "            continue\n",
    "        else:\n",
    "            second = matched.iloc[0].unmin_mol\n",
    "            synthon_name = matched.iloc[0]['name']\n",
    "        for i, smiles in enumerate(combodex['mergers'][synthon_smiles]):\n",
    "            name = f'{first_name}-{synthon_name}-{i}'\n",
    "            data.append(dict(name=name, hits=[first, second],\n",
    "                             primary_name=first_name, secondary_parent=second_name, secondary_name=synthon_name,\n",
    "                             smiles=smiles.replace('Xe', 'H')))\n",
    "tabular_combinations = pd.DataFrame(data)\n",
    "\n",
    "# -------------------------\n",
    "## Place enumerations\n",
    "\n",
    "lab = Laboratory(pdb_block, None)\n",
    "Victor.monster_throw_on_discard = True\n",
    "placed = lab.place(tabular_combinations, n_cores=20, expand_isomers=True)\n",
    "with gzip.open('placed.pkl.gz', 'wb') as fh:\n",
    "  placed.to_pickle(fh)\n",
    "placed.sort_values('âˆ†âˆ†G', ascending=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-py310-py",
   "language": "python",
   "display_name": "Python [conda env:py310]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
