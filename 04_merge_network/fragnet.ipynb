{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b1dd5c-4883-4f9d-b424-03482c4fb3b0",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m pip install git+https://github.com/stephwills/fragment_network_merges.git\n",
    "python -m pip install joblib neo4j im-data-manager-job-utilities pebble\n",
    "sudo apt-get install ffmpeg libsm6 libxext6  -y\n",
    "export KUBECONFIG=$HOME/config-fragnet\n",
    "export NEO4J_USER=matteo\n",
    "export NEO4J_PASS=ðŸ‘¾ðŸ‘¾ðŸ‘¾\n",
    "export USE_NEO4J_INSTEAD_API=true\n",
    "kubectl port-forward -n graph-b graph-0 7474:7474 &\n",
    "kubectl port-forward -n graph-b graph-0 7687:7687 &\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ae5eb1c-6b2e-40bf-ae38-fb950223a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KUBECONFIG']=os.environ['HOME'] + '/config-fragnet'\n",
    "os.environ['NEO4J_USER']='matteo'\n",
    "os.environ['NEO4J_PASS']='ðŸ‘¾ðŸ‘¾ðŸ‘¾'\n",
    "os.environ['USE_NEO4J_INSTEAD_API']='true'\n",
    "\n",
    "os.environ['FRAGALYSIS_DATA_DIR'] = os.path.join(os.getcwd(), 'fauxalysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12673b1-e091-4299-a2bc-e3abeaf9bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import hits\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "with Chem.SDMolSupplier('filtered_hits.sdf') as sds:\n",
    "    hits = list(sds)\n",
    "    \n",
    "hit_names = [hit.GetProp('_Name') for hit in hits]\n",
    "hit_smileses = [Chem.MolToSmiles(hit) for hit in hits]\n",
    "hitdex = dict(zip(hit_names, hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a41a30-41ac-4974-8947-5d95ec5a42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fauxalysis\n",
    "from typing import List\n",
    "\n",
    "def make_fauxalysis(hits: List[Chem.Mol], target_name: str, base_folder='.') -> None:\n",
    "    if not os.path.exists(base_folder):\n",
    "        os.mkdir(base_folder)\n",
    "    os.environ['FRAGALYSIS_DATA_DIR'] = base_folder\n",
    "    for hit in hits:\n",
    "        hit_name: str = hit.GetProp('_Name')\n",
    "        hit_path = os.path.join(base_folder, f'{target_name}', 'aligned',f'{target_name}-{hit_name}')\n",
    "        os.makedirs(hit_path, exist_ok=True)\n",
    "        Chem.MolToMolFile(hit, os.path.join(hit_path, f'{target_name}-{hit_name}.mol'))\n",
    "           \n",
    "target_name = 'EV-D68-protease'\n",
    "make_fauxalysis(hits, target_name, os.path.join(os.getcwd(), 'fauxalysis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a33f80-69bc-43ac-9618-62fd718c6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 fragments removed from list. 21 fragments remaining.\n",
      "Expanding fragment A: x1498L0AP1 with synthons of fragment B: x1498L0AP2\n",
      "Found 2 synthons\n",
      "1 synthons remaining after filtering\n",
      "Running synthon 0\n"
     ]
    }
   ],
   "source": [
    "## Search\n",
    "\n",
    "from merge import query\n",
    "from merge.find_merges import getFragmentNetworkSearcher\n",
    "from merge.find_merges_generic import MergerFinder_generic  # solely for typehinting\n",
    "\n",
    "searcher: MergerFinder_generic = getFragmentNetworkSearcher()\n",
    "\n",
    "valid_smileses, valid_names = searcher.filter_for_nodes(hit_smileses, hit_names)\n",
    "smiles_pairs, name_pairs = searcher.get_combinations(valid_smileses, valid_names)\n",
    "all_mergers: List[Dict] = []\n",
    "for smiles_pair, name_pair in zip(smiles_pairs, name_pairs):\n",
    "    mergers: Dict[str, List[str]] = searcher.get_expansions(smiles_pair, name_pair, target_name, 'output')\n",
    "    all_mergers.append(dict(mergers=mergers, smiles_pair=smiles_pair, name_pair=name_pair))\n",
    "\n",
    "with gzip.open('fragnet_pre.pkl.gz', 'wb') as fh:\n",
    "    pickle.dump(all_mergers, fh)\n",
    "    \n",
    "print(len(all_mergers),\\\n",
    "sum([len(m['mergers']) for m in all_mergers]), \\\n",
    "sum([len(mm) for m in all_mergers for mm in m['mergers'].values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c8456-6e16-4af8-b220-66afa5549794",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse synthons\n",
    "\n",
    "import operator\n",
    "import pandas as pd\n",
    "from fragmenstein import Victor, Laboratory, Igor\n",
    "\n",
    "dfs = [ pd.DataFrame([{'smiles': synthon.replace('Xe', 'H'),\n",
    "                       'original_name': f'{merge_info[\"name_pair\"][1]}-synthon{i}',\n",
    "                       'xenonic': synthon,\n",
    "                       'parent': merge_info['name_pair'][1],\n",
    "                       'hits': [hitdex[merge_info['name_pair'][1]]]} for i, synthon in enumerate(merge_info['mergers'].keys())])\n",
    "       for merge_info in all_mergers\n",
    "      ]\n",
    "\n",
    "synthons = pd.concat(dfs, axis='index')\n",
    "\n",
    "# fix duplicated\n",
    "synthons['inchi'] = synthons.smiles.apply(Chem.MolFromSmiles).apply(Chem.RemoveAllHs).apply(Chem.MolToInchiKey)    \n",
    "synthons = synthons.drop_duplicates(['parent', 'inchi'])\n",
    "synthons['name'] = synthons.parent +'S'+ (synthons.groupby(['parent']).cumcount()+1).astype(str)\n",
    "Igor.init_pyrosetta()\n",
    "placed_synthons = Laboratory(pdbblock=pdb_block, covalent_resi=None).place(synthons, n_cores=55)\n",
    "\n",
    "def fix_name(row):\n",
    "    # error... min_mol has it. not unmin.\n",
    "    mol = Chem.Mol(row.unmin_binary)\n",
    "    mol.SetProp('_Name', row['name'])\n",
    "    return mol\n",
    "    \n",
    "synthons['âˆ†âˆ†G'] = placed_synthons['âˆ†âˆ†G']\n",
    "synthons['unmin_mol'] = placed_synthons.apply(fix_name, axis=1)\n",
    "from rdkit.Chem import PandasTools\n",
    "PandasTools.WriteSDF(df=synthons,\n",
    "                     out='fragnet-synthons.sdf',\n",
    "                     molColName='unmin_mol', \n",
    "                     idName='name',\n",
    "                     properties=['parent', 'âˆ†âˆ†G'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fee0e1-c3a9-4661-9676-d1ea5cc341e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915ed8c-b612-4667-877f-e2f31dd641c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix names of synthons in combination and make it a long table\n",
    "data = []\n",
    "combodex: dict\n",
    "for combodex in all_mergers:\n",
    "    # 'mergers', 'smiles_pair', 'name_pair'\n",
    "    first_name, second_name = combodex['name_pair']\n",
    "    first: Chem.Mol = hitdex[first_name]\n",
    "    for synthon_smiles in combodex['mergers']:\n",
    "        clean_smiles = synthon_smiles.replace('Xe', 'H')\n",
    "        inchi = Chem.MolToInchiKey( Chem.RemoveAllHs( Chem.MolFromSmiles(clean_smiles) ) )\n",
    "        matched = placed_synthons.loc[(placed_synthons['parent'] == second_name) & (placed_synthons.inchi == inchi)]\n",
    "        if len(matched) == 0:\n",
    "            print(first_name, second_name, synthon_smiles, 'missing!')\n",
    "            # Z2111637360\n",
    "            second = hitdex[second_name]\n",
    "            synthon_name = second_name+'X'\n",
    "        elif matched.iloc[0]['âˆ†âˆ†G'] > -1.:\n",
    "            # skip crap floater fragments\n",
    "            continue\n",
    "        else:\n",
    "            second = matched.iloc[0].unmin_mol\n",
    "            synthon_name = matched.iloc[0]['name']\n",
    "        for i, smiles in enumerate(combodex['mergers'][synthon_smiles]):\n",
    "            name = f'{first_name}-{synthon_name}-{i}'\n",
    "            data.append(dict(name=name, hits=[first, second], \n",
    "                             primary_name=first_name, secondary_parent=second_name, secondary_name=synthon_name,\n",
    "                             smiles=smiles.replace('Xe', 'H')))\n",
    "tabular_combinations = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c2bbe-d492-4ab8-9b2d-2b33a3bccfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Place enumerations\n",
    "    \n",
    "lab = Laboratory(pdb_block, None)\n",
    "Victor.monster_throw_on_discard = True\n",
    "placed = lab.place(tabular_combinations, n_cores=55, expand_isomers=True)\n",
    "with gzip.open('fragnet.placed.pkl.gz', 'wb') as fh:\n",
    "  placed.to_pickle(fh)\n",
    "placed.sort_values('âˆ†âˆ†G', ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
